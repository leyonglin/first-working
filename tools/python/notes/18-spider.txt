

01： 爬虫的基础 分类 用途 法律风险 爬取网站前的准备工作 爬虫的广度优先简介 深度
优先策略 

django部署：
	uwsgi:web服务器与web框架之间一种简单而通用的接口
		1.确保Django项目能够运行
			python3 manager.py runserver 0.0.0.0:8888
		2.安装web服务器
			1.安装
			2.启动
			3.验证：访问127.0.0.1
		3.安装uwsgi
			1.安装：pip3 install uwsgi
			2.验证: uwgsi --http :9000 --chdir project_path --module project_Directory/wsgi.py
				访问：127.0.0.1:9000
	
	项目部署
		1.配置uwsgi(配置文件)和nginx通信端口  启动端口
			1.在项目根目录(manager.py所在目录)中新建uwsgi启动文件:pro_nameUwsgi.ini
				cat fruitdayUwsgi.ini
					[uwsgi]
					#指定和nginx通信端口
					socket=127.0.0.1:8001
					#项目路径
					chdir=根目录
					#wsgi.py路径
					wsgi-file=project_Directory/wsgi.py
					#进程数
					processes=4
					#线程数
					thread=2
					#uwsgi自身占用端口
					stats=127.0.0.1:8080
			2.配置nginx配置文件
				cat uwsgi.conf
					server{
						listen 80;
						server_name www.uwsgi.com;
						#指定字符集
						charset utf-8;
						#指定收集静态文件路径
						location /static{
							alias 项目根目录/static;
						}
						#和uwsgi通信端口和通信文件
						localtion /{
							include uwsgi_params;
							uwsgi_pass 127.0.0.1:8001;
						}
					}
				拷贝uwsgi_params到项目根目录
				修改默认访问路径并启动服务
				
			3.收集静态文件
				1.在setting.py文件中添加路径(STATIC_ROOT)
					STATIC_ROOT="项目根目录/static"
				2.python3 manager.py collectstatic
	
			4.uwsgi启动项目
				cd 项目根目录
				uwsgi --ini  project_Directory/pro_nameUwsgi.ini


1.网络爬虫
	1.网络蜘蛛/网络机器人，抓取网络数据的程序
	2.总结：用python程序模仿人类去访问网站，模仿的越逼真越好
	3.用途：数据分析
		python：请求模块，解析模块丰富成熟，强大的scrapy框架
		java：代码笨重，代码量大
		php：对多线程，异步支持不是很好
		c++：效率高但代码成型慢
	4.爬虫分类
		1.通用网络爬虫(搜索引擎，遵守robots协议)
			robots协议：https://www.taobao.com/robots.txt
			1.搜索引擎如何获取1个新网址的url
				1.网站主动向搜索引擎提供(百度站长平台)
				2.和DNS服务商(万网)合作，快速收录新网站
		2.聚焦网络爬虫
			面向需求的爬虫：自己写的爬虫程序
	5.数据爬取步骤
		1.确定要爬取的url地址
		2.向网站发请求获取相应的html页面
		3.提取html页面中有用的数据
			1.所需数据，保存
			2.页面中新的url，继续第二步
	6.Anaconda和spyder
		1.Anaconda：科学计算的继承开发环境(集成多个库)
		2.spyder：开发工具(编辑器)
			常用快捷键
				1.注释/取消注释：C+1
				2.运行程序：F5
				3.自动补全：tab
	7.chrome插件
		JSONView
		Proxy SwitchyOmega
		XPath Helper
	8.GET和POST
		1.GET：查询参数会在URL地址上显示，进行拼接
		2.POST：查询参数和需要提交的数据隐藏在Form表单中，不会在URL中显示
		3.url：  #瞄点   ?后面带参数   &参数分隔号
		4.User-Agent
			记录了用户的浏览器，操作系统等
	9.请求模块：
		1.模块：urllib.request
		2.常用方法：
			1.urllib.request.urlopen("url地址")
				作用：向网站发起一个请求，并获取响应
			2.urllib.request.Request({User-Agent})
				1.使用流程
					1.利用Request()方法构建请求对象
					2.利用urlopen()方法获取响应对象
					3.利用响应对象的read().decode("utf-8")获取内容
				2.参数
					1.url
					2.headers = {"User-Agent":" "}
			3.响应对象(res)的方法
				1.res.read():读取服务器响应的内容
				2.res.getcode():返回HTTP的响应码
				3.geturl()
					返回实际数据的URL
		3.urllib.parse模块：url编码模块
			作用：将url的汉字进行编码，使浏览器能识别，有多个参数，会自动用&拼接
			1.urlencode(字典)：{"wd":"美女"}
				例如：urllib.parse.urlencode({"wd":"美女","pn":50})
			2.quote("字符串")
			3.unquote("字符串")



请求方式及参数
	GET：查询参数会在URL地址上显示
	POST
		1.特点：URL地址无变化，数据是在Form表单中
		2.data：表单数据要以bytes类型提交，不能是string
		3.处理表单数据为bytes数据类型
			1.把form表单数据定义为1各大字典
			2.urlencode(data).encode("utf-8")
				先编码得到字符串，再转码得到bytes数据类型
	JSON模块
		1.json.loads(json格式的字符串)
			把json格式的字符串转为python中的字典
			json.loads("key":"value")

正则表达式(re解析模块)
	1.re使用流程
		1.re使用方法1
			1.创建编译对象：p = re.compile(r'正则表达式')
				转义实例：或者直接加r就行了
					import re
					#理解1：首先python会对'\\\\'进行转义，得到正则表达式"\\"，在正则表达式里，要匹配\是要转义，即正则表达式要是"\\"
					#理解2：要匹配字符串中的"\",由于"\"是特殊字符，因此正则表达式必须是"\\"才能匹配的上。但是，由于"\"在python中也是特殊字符，需要对每个"\"转义,因此变成"\\\\"
					rlist=re.findall('\\\\',"aaa\dbbb")
					print(rlist)
					#得到结果是['\\']，即匹配成功(python输出"\"，由于"\"是特殊字符，因此python输出时会对"\"进行转义)
			2.匹配字符串：rlist=p.findall("key":"value")
		2.re使用方法2
			rlist=re.findall(r'正则表达式',html)
	2.表达式
		a  						单个字符，包括汉字    
		|  						或(已匹配过的字符，就不会再匹配)
		.  						匹配除换行外的任意字符
		^  						匹配字符串的开始位置
		$  						匹配字符串的结束位置
		*  						匹配前一个字符出现0次或多次
		+  						匹配前一个字符出现1次或多次
		？ 						匹配前一个字符出现0次或1次
		{n}						匹配n次重复次数
		{m,n}       			匹配m到n次重复次数(包含mn)
		.+/.*       			匹配全部
		[字符集]    			匹配字符集中任意一个字符([_a-zA-Z0-9])
		[^字符集]   			过滤，即除字符集外的任意字符
		\d == [0-9]				匹配任意数字字符
		\D == [^0-9]			匹配任意非数字字符
		\w                      普通字符(字母数字下划线及汉字)
		\W						非普通字符
		\s						匹配任意空字符[\r\t\n\v\f ]
		\S                      匹配任意非空字符
		\A == ^                 匹配字符串的开始位置
		\Z == $                 匹配字符串的结束位置
		\b						匹配单词边界位置(普通字符和非普通字符交界认为是单词边界)
		\B						匹配非单词边界位置

		匹配所有字符的方式:
			[\s\S]*
			.*,re.S(作用：匹配\n在内的所有字符)







02： HTTPS协议解析 用抓包工具抓取分析网络数据包 BeautifulSoup XPath 

1.数据分类
	1.结构化的数据
		特点：有固定的格式(html/xml/json)
	2.非结构化数据
		一般为二进制
2.常用匹配:
	p = re.compile(r"正则表达式",re.S)
	robj = p.match(html)              #匹配字符串开头
	r = r.group()
3.常用方法：
	1.findall(html)      所有全部匹配，返回1个列表
	2.match(html)        匹配字符串开头，返回对象
	3.search(html)       从开始匹配，匹配到第一个结束
	4.对象.group()       从match或search返回对象中取结果


4.CSV模块使用流程
	1.打开csv文件
		with open("测试.csv","w",newline="",encoding="") as f:
	2.初始化写入对象
		writer=csv.writer(f)
	3.写入数据
		writer.writerow(列表)
		...
5.数据持久化存储
	1.mongodb
	2.mysql
	3.Anaconda安装
		conda install pymongo
6.远程连接数据库
	1.bind-address = 0.0.0.0
	2.添加用户
		grant all privileges on *.* to "user@ip" identified by "passwd" with grant option;
	3.防火墙





03： HTTP协议的GET POST方法在爬虫中的使用 动态网站的Selenium+浏览器方案抓取 代理服务器的使用 

http协议是一种无连接协议，客户端和服务端交互仅局限于请求/响应，为了维护连接，客户端在cookie保存用户信息
	cookie：客户端信息保存用户身份
	session：服务端信息确认用户身份
	headers = {"User-Agent":"Mozilla/5.0"
		"cookie":""
		}


requests模块(第三方模块)
	1.常用方法
		1.get(url,headers=headers)
	2.响应对象res属性
		1.encoding：响应编码
		  res.encoding = "utf-8"
		2.text  :字符串
		3.content ：bytes(非结构化数据/二进制)
		4.status_code:HTTPS响应码
		5.url:返回实际数据的URL地址
	3.get()方法中参数
	res = requests.get(url,params,headers,proxies,auth,verify)
		1.查询参数:字典
			网址：https://www.baidu.com/s?wd=美女&pn=90
				params = {
					"wd":key,
					"pn":pn,
					}   				#会自动编码和拼接
				res = requests.get(url,params=params,headers=headers)
				
		2.使用代理(proxies)
			快代理/全网代理
			透明/高匿
			1.格式： 			
			#普通代理
				proxies = proxies={"协议":"协议://IP:端口号"}
			#私密代理
				proxies = {"http":"http://用户名:密码@IP地址:端口号"}
			验证：http://httpbin.org/get	
		3.web客户端验证(auth)
			1.auth = ("用户名","密码")

		4.SSL证书认证(verify)
			1.verify = True(默认)：进行CA证书认证
			2.verify = False: 不进行认证      #参数为True进行认证，访问https网站(没有进行CA认证)，抛出异常：SSLError

	4.post
		1.requests.post(url,data=data,headers=headers,...)    #其它get参数也可以用
			data:字典，Form表单数据，不用编码，不用转码
		
xpath工具(解析)
	1.在XML文档中查找信息的语言，同样适用于HTML文档的搜索
	2.xpath辅助工具
		1.Chrome插件：XPath Helper
			打开/关闭:C+S+x
		2.Firefox插件：XPath Checker
		3.XPath编辑工具：XML quire(不常用)
			1.//:从整个文档中查找节点
			2.@:选取某个节点的属性值
				//title[@lang="en"] 或者 //tite[@lang] 或者 //title/@lang
			3.匹配多路径
				1.xpath表达式1 | xpath表达式2
				2.获取所有book节点下的title子节点和price子节点
					//book/title | //book/price
			4.函数
				1.contains():匹配一个属性值中包含某些字符串的节点
					//title[contains(@lang,"e")]
				2.text()：获取标签之间文本(<a href="/" id="channel">新浪社会</a>的新浪社会)
					//title[contains(@lang,"e")]/text()

			
				#查找所有book节点一下的title子节点中，lang属性为"en"的节点
					//book/title[@lang="en"]  
				#查找bookstore下的第2个book节点下的title子节点
					/bookstore/book[2]/title
	
lxml库及xptath使用
	1.使用流程
		1.导模块
			from lxml import etree
		2.创建解析对象
			parseHtml=etree.HTML(html)
		3.调用xpath
			rlist=parseHtml.xpath('xpath表达式')
			
			

抓包工具fiddler
	1.配置：Tools -- HTTPS -- 1.Decrypt HTTPS traffic(抓取https包)/Actions(信任root证书) -- 2.from browsers only(只抓取浏览器的包) -- 3.connections(代理端口)
		右半边---Inspectors --1. Headers(请求头数据)  -- 2.webforms(post) -- 3.raw(将请求头转换成纯文本)
	2.浏览器配置：switchyomega(浏览器插件) -- 选项 --新建情景模式 -- 代理服务器和代理端口 -- 应用选项  然后，在插件出选择新建的情景模式
		如果关闭fiddler的时候，需要将switchyomega切换成系统代理



ubuntu防火墙：ufw




04： Cookie Session的使用 Cookiejar的管理 表单提交 

Handler处理器(urllib.request)
	1.使用流程
		1.创建Handler对象
			phandler = urllib。request。Proxy Handler({字典})
		2.创建opener对象
			opener = urllib.request.build_opener(phandler)
		3.发请求获响应
			req = urllib.request.Request(url,headers=headers)
			res=opener.open(req)
	2.分类
		1.ProxyHandler({普通代理字典})
		2.ProxyBasicAuthHandler(密码管理器对象)
		3.HTTPBasicAuthHandler(密码管理器对象)
		4.流程
			1.密码管理器对象操作
				1.pwdmg = urllib.request.HTTPPassword...
				2.pwding.add_password(None,{"http":"IP:端口"},"user","pwd"






05： 数据的持续化存储 数据库的使用 多进程 多线程在爬虫框架中的使用 


Ajax动态加载
	1.抓包工具：WebForms -> QueryString
	2.parmas = {QueryString一堆查询参数}
	3.URL地址：抓包工具Raw下的GET地址



selenium + phantomjs/chromedriver 强大网络爬虫组合
	1.selenium(模块)
		1.web自动化测试工具，应用于web自动化测试
		2.特点：
			1.可以运行在浏览器，根据指定命令操作浏览器，让浏览器自动加载页面
			2.需要与第三方浏览器结合使用
	2.phantomjs（应用程序）
		1.无界面浏览器(无头浏览器)
		2.特点：
			1.把网站加载到内存进行页面加载
			2.运行高效
	3.chromedriver可设置有/无界面（程序）
		chromdriver.storage.googleapis.com(需要与谷歌浏览器版本一致)
	4.打开百度/发送文字并点击，最后获取页面截图
	5.浏览器对象(driver)的方法
		1.driver.get(url):发请求，或响应
		2.driver.page_source:获取html源码
		3.driver.page_source.find('字符串'):从源码找字符串，查找失败返回-1
	6.单元素查找（节点对象，只查找第一个search）
		1.driver.find_element_by_id('')
		2.driver.find_element_by_name('')
		3.driver.find_element_by_class_name('')
		4.driver.find_element_by_xpath('')
	7.多元素查找
		1.driver.find_elements_by_id('')
		2.driver.find_elements_by_name('')
		3.driver.find_elements_by_class_name('')
		4.driver.find_elements_by_xpath('')
		返回值是一个列表
	8.节点对象.send_keys('')     #发送文字
	9.节点对象.click()			 #点击
	10.chromdriver参数
		百度  "ChromeOptions()"




06： Scrapy框架的使用以及如何对爬虫进行分页 去重 

进程：系统正在运行的一个程序
	1个cpu在同一时间只能执行一个进程
线程：一个进程可包含多个线程
	GIL(全局解释锁)导致1个cpu只能运行一个线程
	
1.队列(from queue import Queue)
	put()
	get()
	Queue.empty():是否为空
	Queue.join():如果队列为空，执行其它程序
2.线程(import threading)
	threading.Thread(target=...)


BeautifulSoup解析
	1.定义：HTML或XML的解析器，依赖于lxml
	2.安装：python -m pip install beautifulsoup4
	3.使用流程：
		1.导模块
			from bs4 import BeautifulSoup
		2.创建解析对象
			soup = BeautifulSoup(html,'lxml')
		3.查找节点对象(列表)
			soup.find_all("div",attrs={"class":"test"})
	4.BeautifulSoup支持的解析库
		1.lxml:soup = BeautifulSoup(html,"lxml")  #筛选速度快，容错率强
		2.html.parser:python标准库				  #一般
		3.xml：速度快								
	5.节点选择器
		1.选择节点并获取内容
			节点对象.节点名.string
	6.find_all():返回列表


scrapy框架
	1.定义：异步处理框架，可配置和可扩展程度非常高
	2.scrary startproject pro_name
	3.scrapy组件：
		 爬虫程序：发送url，解析返回内容
		1.scrapy引擎(engine):框架的核心
		2.调度器(scheduler):接收从引擎发过来的URL，入队列
		3.下载器(downloader):获取网页源码，返回爬虫程序
		4.下载器中间件(downloader middlewares)）：在引擎去往下载器的中间
			蜘蛛中间件(spider middlewares)：在引擎返回爬虫程序的中间
		5.项目管道(item pipeline):数据处理

	4.制作scrapy爬虫项目的步骤
		1.新建项目,执行命令
			scrapy startproject pro_name
		2.明确目标(items.py)
			编辑items.py文件
				name = scrapy.Field()
		3.制作爬虫程序
			进入导spiders文件夹中，执行命令:
				scrapy genspider 文件名 "域名"
		4.处理数据(pipelines.py)
		5.配置settings.py
		6.运行爬虫程序
			1.scrapy crawl 爬虫名
			2.保存为csv文件：scrapy crawl 爬虫名 -o 文件名.csv
			
	5.scrapy项目文件讲解

		Baidu/   						#执行命令scrapy startproject Baidu
		├── Baidu                       #项目目录
		│   ├── __init__.py
		│   ├── items.py				#定义爬取的数据结构
		│   ├── middlewares.py			#下载器中间件和蜘蛛中间件
		│   ├── pipelines.py			#管道文件，处理数据
		│   ├── settings.py				#项目全局配置
		│   └── spiders					#文件夹，存放爬虫程序
		│       ├── baiduspider.py		#爬虫程序，执行命令：scrapy genspider baiduspider  "www.baidu.com"
		│       └── __init__.py
		└── scrapy.cfg					#基本配置文件，一般不用改

	6.运行流程
		1.Engine想spider索要URL(爬虫文件的第一个start_urls)
		2.交给Scheduler入队列
		3.scheduler出队列，通过downloader middlewares交给downloader
		4.下载完成，通过spider middlewares给spider
		5.spider做数据提取
			1.把数据交给Item Pipeline 
			2.把需要跟进url交给scheduler入队列
		6.当scheduler中没有任何request请求后，程序结束
		






07： 爬虫项目实战：猫眼 豆瓣电影数据抓取 腾讯招聘网站数据抓取 淘女郎图片抓取

yield：把一个函数当成一个生成器使用，通过对象.__next__()一个一个取出


	网址：https://blog.csdn.net/qq_39305249/article/details/102628783
	爬取标题  时间  数量
	xpath:
		//h1[@class="title-article"]/text()
		//div[@class="article-bar-top"]/span[@class="time"]/text()
		//div[@class="article-bar-top"]/span[@class="read-count"]/text()



1.scrapy startproject Csdn
2.cd Csdn/Csdn
3.scrapy genspider csdn blog.csdn.net
4.修改items.py（爬取数据）
5.修改csdn.py（start_urls）及自定义类匹配出值并return给pipelines.py
6.修改setting.py
7.常见begin.py执行文件(与items同级目录)	
	from scrapy import cmdline
	cmdline.execute('scrapy crawl tengxun'.split())
	cmdline.execute('scrapy crawl tengxun -o 文件名.csv'.split())




1.extract():获取选择器对象中的文本内容
	response.xpath('')  --> 结果:[<selector ...,data='文本内容'>]
	response.xpath('').exreact()[0]  -->结果：文本内容
2.爬虫程序中，start_urls必须为列表
3.pipelines.py中必须有1个函数叫：
	def process_item(self,item,spider):
		pass(处理item的)

4.58招聘项目(数据持久化存储)
	1.网站：https://bj.58.com/kefu/
			https://bj.58.com/kefu/  pn1/
	2.xpath匹配
		表达式：//ul[@id="list_con"]/li
		岗位：//span[@class="name"]
		地点：//span[@class="address"]		
		薪资：//p[@class="job_salary"]
		待遇：//div/span[2]
		链接：//a/@href

5.日志级别及保存日志文件
	LOG_LEVEL=""
	LOG_FILE='文件名.log'
		LOG_ENABLED 默认: True，启用logging	
		LOG_ENCODING 默认: ‘utf-8’，logging使用的编码	
		LOG_FILE 默认: None，在当前目录里创建logging输出文件的文件名
		LOG_LEVEL 默认: ‘DEBUG’，log的最低级别
	5层警告级别
		CRITICAL - 严重错误
		ERROR - 一般错误
		WARNING - 警告信息
		INFO - 一般信息
		DEBUG - 调试信息

ajax：可以查看F12的xhr


6.保存为csv或json文件
	保存为csv/json文件：scrapy crawl 爬虫名 -o 文件名.csv/json
	settings.py
		FEED_EXPORT_ENCODING = 'utf8'   设置导出格式
	设置导出空行问题：修改源码(百度)


7.盗墓笔记：http://www.daomubiji.com/dao-mu-bi-ji-1
	基准：
		response.xpath('//article/a/text()').extract()
		for r in []:
			标题
			章节数
			章节名称
			链接 '//article/a/@href'[i]
			yield item


8.scrapy shell
	1.用法：scrapy shell URL
		response.headers
		request.headers：请求头
		response.text：string类型
		response.body：bytes类型
		request.xpath('')
	2.scrapy.Request()常用参数:
		request.meta
			定义代理等参数相关信息
			不同请求之间传递数据
		dont_filter:是否忽略域组限制
			默认False：检查allowed_domains
		encoding:默认utf-8
		callback: 指定解析函数
9.下载器中间件（随机User-Agent）
	1.setting.py(适合少量)
		1.User_Agent
		2.DEFAULT_REQUEST_HEADERS={}
	2.middlewares.py设置中间件
		1.创建一个新的存放user_agent的python文件
		2.在middlewares.py中导入并自定义类并重写process_request方法
		3.在settings.py注册
10.下载器中间件（随机代理）
		1.创建一个新的存放user_agent的python文件
        2.在middlewares.py中导入并自定义类并重写process_request方法(和useragent是一样的方法)
        3.在settings.py注册

11.CrawlSpider类
	1.Spider的派生类
		from scrapy.spiders import CrawlSpider
		定义一些规则来提取跟进链接，从爬取的网页中提取链接并继续爬取
	2.提取链接的流程(LinkExtractor)
		1.scrapy shell URL
		2.from scrapy.linkextractors import LinkExtractor
		3.LinkExtractor(allow='').extract_links(response)
	3.创建爬虫文本模板（CrawlSpider类）
		cd Baidu/Baidu(不要与原模版文件名及name重名)
		scrapy genspider -t crawl 爬虫名 域名
12.scrapy redis-key

setting.py配置
	重新配置各模块
		SCHEDULER = "scrapy_redis.scheduler.Scheduler"
	去重
		DUPEFILTER_CLASS="scrapy_redis.dupefilter.RFPDupeFilter"
	保持调度器队列，断点续爬
		SCHEDULER_PERSIST = True
	配置redis数据库链接地址
		REDIS_URL = 'redis://172.16.24.4434223:6379'         


3-02:06













